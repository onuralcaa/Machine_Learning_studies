{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77608698",
   "metadata": {},
   "source": [
    "## Geleneksel makine öğrenme metotları"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74726996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d524af50",
   "metadata": {},
   "source": [
    "### AG News: Haber metinlerinin sınıflandırılması."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5d40dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (file://C:/Users/onura/.cache/huggingface/datasets/parquet/ag_news-f4012edcb412d6fa/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Loading a dataset cached in a LocalFileSystem is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# AG News veri setini yükleme\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_news\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Eğitim veri setinden örnekleme (10.000 örnek)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m train_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\load.py:1810\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1806\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[0;32m   1807\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1808\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[0;32m   1809\u001b[0m )\n\u001b[1;32m-> 1810\u001b[0m ds \u001b[38;5;241m=\u001b[39m builder_instance\u001b[38;5;241m.\u001b[39mas_dataset(split\u001b[38;5;241m=\u001b[39msplit, verification_mode\u001b[38;5;241m=\u001b[39mverification_mode, in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory)\n\u001b[0;32m   1811\u001b[0m \u001b[38;5;66;03m# Rename and cast features to match task schema\u001b[39;00m\n\u001b[0;32m   1812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\builder.py:1107\u001b[0m, in \u001b[0;36mDatasetBuilder.as_dataset\u001b[1;34m(self, split, run_post_process, verification_mode, ignore_verifications, in_memory)\u001b[0m\n\u001b[0;32m   1105\u001b[0m is_local \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m is_remote_filesystem(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fs)\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local:\n\u001b[1;32m-> 1107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading a dataset cached in a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fs)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_dir):\n\u001b[0;32m   1109\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   1110\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: could not find data in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please make sure to call \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilder.download_and_prepare(), or use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets.load_dataset() before trying to access the Dataset object.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1113\u001b[0m     )\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Loading a dataset cached in a LocalFileSystem is not supported."
     ]
    }
   ],
   "source": [
    "# AG News veri setini yükleme\n",
    "dataset = load_dataset('ag_news')\n",
    "\n",
    "# Eğitim veri setinden örnekleme (10.000 örnek)\n",
    "train_data = pd.DataFrame(dataset['train'])\n",
    "train_data_sampled = train_data.sample(10000, random_state=42)\n",
    "\n",
    "# Test veri seti\n",
    "test_data = pd.DataFrame(dataset['test'])\n",
    "\n",
    "# Metin ve etiketleri ayırma\n",
    "X_train_texts = train_data_sampled['text']\n",
    "y_train = train_data_sampled['label']\n",
    "X_test_texts = test_data['text']\n",
    "y_test = test_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47b5414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri seti bilgilerini yazdırma\n",
    "for split in dataset.keys():\n",
    "    print(f\"--- {split.upper()} ---\")\n",
    "    print(f\"Örnek sayısı: {len(dataset[split])}\")\n",
    "    \n",
    "    # Ortalama metin uzunluğu\n",
    "    total_text_length = sum(len(item['text']) for item in dataset[split])\n",
    "    avg_text_length = total_text_length / len(dataset[split])\n",
    "    print(f\"Toplam metin uzunluğu: {total_text_length}\")\n",
    "    print(f\"Ortalama metin uzunluğu: {avg_text_length:.2f} karakter\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e26ed72",
   "metadata": {},
   "source": [
    "### Embedding yöntemi olarak TF-IDF kullanılmıştır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce406212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF ile metinleri vektörleştirme (özellik sayısı 1000 ile sınırlandırıldı)\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X_train = vectorizer.fit_transform(X_train_texts)\n",
    "X_test = vectorizer.transform(X_test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d9dcf2",
   "metadata": {},
   "source": [
    "### Geleneksel Makine Öğrenme Metotları"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6069c728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modellerin tanımlanması\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"SVM\": SVC(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=10, n_jobs=-1)  # Paralelleştirme\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "# Her model için eğitim ve değerlendirme\n",
    "for name, model in models.items():\n",
    "    # Modeli eğit\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Test verisi üzerinde tahmin yap\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Performans metriklerini hesapla\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Precision\": precision_score(y_test, y_pred, average='weighted'),\n",
    "        \"Recall\": recall_score(y_test, y_pred, average='weighted'),\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"F-Score\": f1_score(y_test, y_pred, average='weighted')\n",
    "    })\n",
    "\n",
    "# Sonuçları tablo olarak göster\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d3d04b",
   "metadata": {},
   "source": [
    "-----------\n",
    "## Derin Öğrenme Metotları"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bece737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, LSTM, Dropout\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171b2532",
   "metadata": {},
   "source": [
    "#### Word2Vec ile CNN ve LSTM Modelleri:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c381b234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eğitim verisini kelime listelerine ayırma\n",
    "X_train_tokens = [sentence.split() for sentence in X_train_texts]\n",
    "X_test_tokens = [sentence.split() for sentence in X_test_texts]\n",
    "\n",
    "# Word2Vec modeli eğitme\n",
    "word2vec_model = Word2Vec(sentences=X_train_tokens, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Kelime vektörlerini içeren bir sözlük\n",
    "word_vectors = word2vec_model.wv\n",
    "vocab_size = len(word_vectors)\n",
    "print(f\"Kelime vektörlerinin boyutu: {word_vectors.vector_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0731cfd4",
   "metadata": {},
   "source": [
    "#### Embedding Matrisini Hazırlama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056307e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding matrisini oluşturma\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((vocab_size + 1, embedding_dim))  # +1, bilinmeyen kelimeler için\n",
    "word_index = {word: idx + 1 for idx, word in enumerate(word_vectors.index_to_key)}\n",
    "\n",
    "for word, idx in word_index.items():\n",
    "    embedding_matrix[idx] = word_vectors[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c51469",
   "metadata": {},
   "source": [
    "#### Metinleri sayısal formata çevirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acc1e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kelimeleri dizilere dönüştürme\n",
    "def text_to_sequence(tokens, word_index):\n",
    "    return [[word_index.get(word, 0) for word in sentence] for sentence in tokens]\n",
    "\n",
    "X_train_seq = text_to_sequence(X_train_tokens, word_index)\n",
    "X_test_seq = text_to_sequence(X_test_tokens, word_index)\n",
    "\n",
    "# Dizileri aynı uzunluğa getirerek doldurma\n",
    "max_length = 100\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fedbec",
   "metadata": {},
   "source": [
    "### CNN Modeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca773b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN modeli\n",
    "cnn_model = Sequential([\n",
    "    Embedding(input_dim=vocab_size + 1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False),\n",
    "    Conv1D(128, 5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(4, activation='softmax')  # 4 sınıf\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "cnn_model.summary()\n",
    "\n",
    "# CNN modelini eğitme\n",
    "cnn_model.fit(X_train_padded, y_train, epochs=5, batch_size=64, validation_data=(X_test_padded, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebab570",
   "metadata": {},
   "source": [
    "### LSTM Modeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f415919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM modeli\n",
    "lstm_model = Sequential([\n",
    "    Embedding(input_dim=vocab_size + 1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.5),\n",
    "    LSTM(64),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(4, activation='softmax')  # 4 sınıf\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "lstm_model.summary()\n",
    "\n",
    "# LSTM modelini eğitme\n",
    "lstm_model.fit(X_train_padded, y_train, epochs=5, batch_size=64, validation_data=(X_test_padded, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafdf1c5",
   "metadata": {},
   "source": [
    "### Performans değerlendirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae6ba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN performans değerlendirme\n",
    "cnn_preds = np.argmax(cnn_model.predict(X_test_padded), axis=1)\n",
    "cnn_precision = precision_score(y_test, cnn_preds, average='weighted')\n",
    "cnn_recall = recall_score(y_test, cnn_preds, average='weighted')\n",
    "cnn_accuracy = accuracy_score(y_test, cnn_preds)\n",
    "cnn_f1 = f1_score(y_test, cnn_preds, average='weighted')\n",
    "\n",
    "print(f\"CNN -> Precision: {cnn_precision}, Recall: {cnn_recall}, Accuracy: {cnn_accuracy}, F-Score: {cnn_f1}\")\n",
    "\n",
    "# LSTM performans değerlendirme\n",
    "lstm_preds = np.argmax(lstm_model.predict(X_test_padded), axis=1)\n",
    "lstm_precision = precision_score(y_test, lstm_preds, average='weighted')\n",
    "lstm_recall = recall_score(y_test, lstm_preds, average='weighted')\n",
    "lstm_accuracy = accuracy_score(y_test, lstm_preds)\n",
    "lstm_f1 = f1_score(y_test, lstm_preds, average='weighted')\n",
    "\n",
    "print(f\"LSTM -> Precision: {lstm_precision}, Recall: {lstm_recall}, Accuracy: {lstm_accuracy}, F-Score: {lstm_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90abc53e",
   "metadata": {},
   "source": [
    "-------\n",
    "## Transfer Öğrenme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d20dce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b24ea4a",
   "metadata": {},
   "source": [
    "### Tokenizer ve model hazırlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2873b11",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Model ve tokenizer\n",
    "model_name = \"bert-base-uncased\"  # İngilizce için temel BERT modeli\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=4, from_pt=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
